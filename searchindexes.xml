<?xml version="1.0" encoding="utf-8" standalone="yes"?><search><entry><title>搭建本地大模型和知识库最简单的方法</title><url>/post/5/</url><categories><category>AI</category></categories><tags><tag>大模型</tag><tag>Ollama</tag><tag>open-webui</tag><tag>anything-llm</tag></tags><content type="html"> 01、本地大模型越来越简单 经过了一年多时间的迭代，大模型种类繁多，使用也越来越简单了。
在本地跑大模型，个人认为目前最好的软件肯定是Ollama无疑了，
不管你是在PC上跑大模型，在Mac上跑大模型，还有在树莓派上跑大模型，
我们都可以用Ollama去跑各种大大小小的模型，而且它的扩展性非常强。
02、Ollama本地运行大模型 现在安装Ollama超级简单的，只需要进入 Ollama官网下载 安装包，然后安装即可。
以下是我个人安装为例（macOS系统）：
1、下载 2、安装 直接点击Next：
3、命令查看 4、运行 其他说明 如果自己不确定模型名称可以去官网查看 模型 每款大模型都有不同版本，根据自己的机器来选择，根据官网的文档也说明了，一般7B的模型至少需要8G的内存，13B的模型至少需要16G内存，70B的模型至少需要64G内存。
03、使用Web UI界面连接Ollama ollama没有界面化的页面使用，它是在终端里交互的，所有我们要使用图形化的界面里去操作，这是我们可以使用Open WebUI。
Open WebUI是一个开源的Web UI界面，github地址：https://github.com/open-webui/open-webui
1、下载和运行 使用Docker方式运行Open WebUI，直接一条命令解决下载和运行：
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main 命令解释：
-d：后台运行 -p 3000:8080：将容器的8080端口映射到主机的3000端口 --add-host=host.docker.internal:host-gateway：添加主机映射，用于访问主机的Docker容器 -v open-webui:/app/backend/data：将主机的open-webui目录映射到容器的/app/backend/data目录 --name open-webui：设置容器名称 --restart always：设置容器总是重 …</content></entry><entry><title>AI的迭代速度不敢想象，学不完根本学不完</title><url>/post/4/</url><categories><category>AI</category></categories><tags><tag>AI工具</tag><tag>AI产品</tag><tag>AI学习资料</tag></tags><content type="html"> 作为一个典型的技术宅程序猿，那对学习技术心态就像蜜蜂采花蜜一样，从浩瀚的知识海洋中汲取精华，从而充实自己的技能库。
在过年期间OpenAI发布了人工智能文生视频大模型Sora时，行业内一片惊讶AI的技术和迭代速度可真快，虽然当时没有产品可以体验，但是以国内的应用能力应该不久后就会有产品推出，离这个消息一个月后有零星几个产品出来，有点不敢想象了。
作为我个人那肯定是默默去学习相关AI知识，作为主技能是Java语言，经历过近几年jdk迭代超级快，一直追着从jdk9学到了jdk21发现这东西迭代太快，好多版本都是一个过度期，学不完根本学不完的。
这次我学习AI就没那么一直追着学习了，从去年3月份就开始学Python基础知识和大模型基础知识，产品不管迭代多快，但最核心知识就是那些。
从去年11月份时，也慢慢学习AI应用工具，让它辅助我更高效学习和应用。毕竟这时的AI应用市场百花齐放，我们大部分人开始不需要使用魔法访问了，国内的产品虽然与国外有点差距，但是对我们普通人应用已经够了，而且AI还在每日不断地迭代更新中。
之前我学习AI知识资料是从飞书一个知识库“通往AGI之路”学习的，现在它都出了网页版 www.waytoagi.com ，那访问体验比在飞书好用多了。
我看到这个主题时，给我感觉就是AI迭代速度不敢想象，学不完根本学不完。
再来看看它的工具分类栏：
因为AI产品的百家争鸣情况，它清晰的为你学习AI应用各自分好类型。
学习AI应用的使用时，也不要忘记每日关注该行业一些动态资讯，它也为我们考虑到了。
时刻了解相关动态把握未来挣钱机会，今年有三种情况的减少，一是薪资减少，二是人工减少，三是利息减少，所以未来挣钱是越来越难，学好相关AI产品应用赋能自己。
它也为我们收集了最新的AI产品和工具，让我们学习不在迷茫，我们可以根据自己情况选择合适自己方向使用相关工具提升自己赋能自己。
应用太多可不要贪杯哟，选择适合自己的，AI迭代速度不可想象，学不完根本学不完的。</content></entry><entry><title>关于我</title><url>/about.html</url><categories/><tags/><content type="html"> AI之路 AIGC已经成为我生活的重要组成部分，我在这里记录了我在AI领域的一些经验，以及我对AI的一些思考。
其他渠道： Github 公众号：小猿编程秘籍</content></entry><entry><title>GPT大模型不再遥不可及：本地化部署让每个人都能拥有</title><url>/post/3/</url><categories><category>AI</category></categories><tags><tag>大模型</tag></tags><content type="html"> 01、本地化部署是GPT发展的一个趋势 我们提到大模型就想到这个东西不是我们普通人可以拥有的，因为太耗费服务器资源，注定了可以提供大模型服务的只能是大厂。
然而有需求就会有解决方案，那就是让大语言模型对特定地区的行业和专业领域有较强的知识储备，使其大而全，变为小而精。无论是医学、法律、金融还是其他行业，搭建专有的知识库解答问题、提供专业建议，就像一个行业内的专家。
本地化部署有以下几个优势：
1、数据完全私有化，降低数据丢失和泄露风险，对数据安全性和私密性有保障。
2、降低使用成本，不需要支付云服务商的订阅费用或按量计费。
3、提高使用灵活性，可以根据自己的需求定制大模型的功能和参数。
4、提高使用效率，不受网络延迟和稳定性的影响。
目前已经有许多支持本地化的大模型，我推荐几个开源的好用的项目：
02、RWKV-Runner 2.1 介绍
RWKV是一个开源且允许商用的大语言模型，灵活性很高且极具发展潜力。
这个工具旨在降低大语言模型的使用门槛，做到人人可用，工具提供了全自动化的依赖和模型管理，你只需要直接点击运行，跟随引导，即可完成本地大语言模型的部署，工具本身体积极小，只需要一个exe即可完成一键部署。
此外，本工具提供了与OpenAI API完全兼容的接口，这意味着你可以把任意ChatGPT客户端用作RWKV的客户端，实现能力拓展，而不局限于聊天。
2.2 功能
RWKV模型管理，一键启动 与OpenAI API完全兼容，一切ChatGPT客户端，都是RWKV客户端。启动模型后，打开 http://127.0.0.1:8000/docs 查看详细内容 全自动依赖安装，你只需要一个轻巧的可执行程序 预设了2G至32G显存的配置，几乎在各种电脑上工作良好 自带用户友好的聊天和续写交互页面 易于理解和操作的参数配置 内置模型转换工具 内置下载管理和远程模型检视 内置一键LoRA微调 也可用作 OpenAI ChatGPT 和 GPT Playground 客户端 多语言本地化 主题切换 自动更新 2.3 界面 2.4 项目地址：https://github.com/josStorer/RWKV-Runner
03、ChatGLM3 3.1 介绍
ChatGLM3 是智谱AI和清华大学 KEG 实验室联合发布的新一代对话预训练模型。ChatGLM3-6B …</content></entry><entry><title>Prompt的指导准则，你学会了吗？</title><url>/post/2/</url><categories><category>AI</category></categories><tags><tag>Prompt</tag></tags><content type="html"><![CDATA[  1、AI名词解释 先来解释一些AI名词：
名词 解释 AGI 通用人工智能 AI 人工智能 AIGC 生成式AI ANI 狭义人工智能 ASI 人工超级智能 Fine-Tuning 微调 Few-Shot 少样本学习 Zero-Shot 零样本学习 2、Prompt Engineering学习 今日重点说说这个 Prompt Engineering（提示工程）：
它是人工智能中的一个概念，特别是自然语言处理（NLP）。
在提示工程中，任务的描述会被嵌入到输入中。
提示工程的典型工作方式是将一个或多个任务转换为基于提示的数据集，并通过所谓的“基于提示的学习（prompt-based learning）”来训练语言模型。
以下举例的测试都是基于国内版以下三款大模型进行测试：
智普清言：https://chatglm.cn/main/detail 文心一言：https://yiyan.baidu.com/ Moonshot：https://kimi.moonshot.cn/chat
你可以选择其他的大模型，下图仅供参考： 指导准则 准则1：尽可能保证下达的指令“清晰、没有歧义” 1、使用分隔符清晰地分清文章内容
在编写 Prompt 时，我们可以使用各种标点符号作为分隔符，比如： ```，&amp;quot;&amp;quot;&amp;quot;，&amp;lt; &amp;gt;， 做分隔符，分隔符作用是可以防止提示词注入，因为用户输入的文本可能包含与你的预设 Prompt 相冲突的内容，如果不加分隔，这些输入就可能注入并操纵语言模型，导致模型产生毫无关联的乱七八糟的输出。
举例：
把用三个引号括起来的文本总结成一句话。 &amp;#39;&amp;#39;&amp;#39; 您应该提供尽可能清晰、具体的指示，以表达您希望模型执行的任务。 这将引导模型朝向所需的输出，并降低收到无关或不正确响应的可能性。 不要将写清晰的提示词与写简短的提示词混淆。 在许多情况下，更长的提示词可以为模型提供更多的清晰度和上下文信息，从而导致更详细和相关的输出。 &amp;#39;&amp;#39;&amp;#39; 2、以一定格式输出内容
有时候我们获取结果时，需要语言模型给我们一定格式的输出，而不是连续的文本。比如：输出CSV、HTML或者Json格式
举例：
请生成包括书名、作者和类别的三本虚构的、非真实存在的中文书籍清单， 并以 JSON 格式提供，其中包含以下 …  ]]></content></entry><entry><title>我可以用AI做什么，帮助自己更强大</title><url>/post/1/</url><categories><category>AI</category></categories><tags/><content type="html"> 引言 人工智能（AI）对话大模型应用ChatGPT发布至今有一年了，但其带来的 AI 浪潮仍然在全球疯狂蔓延。
从今年3月堪称“生成式 AI（AIGC）集体爆发月”。两周内，国外公司争先出场，国内就只有百度的文心一言，
通过我体验使用ChatGPT，让我感觉不可思议，通过扮演不同的角色，回答各类问题、写代码等等，我已经深深感受到，ChatGPT已经让所有创业者焦虑。
从3月至今，国外发展突飞猛进，一步步提升技术，一步步开发应用。在国内ChatGPT在也是紧跟其后，一步步追赶，现在也是各大企业争先出场，推出各类应用。如：AI绘画、AI写作、AI聊天、AI语音合成、AI视频制作、AI智能办公，等等一些辅助我们学习、工作、娱乐、聊天软件。
这些应用使用简单，但是获取符合自己想要的结果是有一定门槛的，这就涉及到如何使用模型能懂的语言与之交流了。
如何去学习对应的知识？如何去应用AIGC？接着就引发我们的思考：“我可以用AI做什么，帮助自己更强大。”
如何去学习AI 大多数人肯定会想到，我们应该从哪里开始学起？
我推荐布鲁姆分类法学习路径：
1、记忆：先从AI的历史、基本术语、重要人物、方法和原理等开始了解
2、理解：进一步了解AI领域的主要思想和概念
3、应用：深入了解Prompt，选择适合自己的AI对话、绘画和语音产品，每天都用它，并使用它们来解决实际问题或提升效率
4、分析：大量阅读各类文章、视频以及行业报告，理解各知识之间的关系
5、评价：通过各类课程与书籍更深入学习，判断信息的价值、提出自己的观点和论证
6、创造：将精选AI网站和APP产品、最新前沿产品都试一试，创造你自己的新想法或产品
人工智能和AI绘画历史 人工智能历史： 1945年：艾伦・图灵 1950年：感知机模型 1956年：达特茅斯会议 1966年：经历低潮 1960-1970年代：早期专家系统 1980年：神经网络 1990-2000年代：机器学习 1997年：深蓝赢得国际象棋比赛 2012年：深度学习兴起、AlexNet赢得ImageNet挑战赛 2016年：AlphaGO战胜围棋世界冠军 2022年：ChatGPT问世 AI绘画历史： 1970年：哈罗德·科恩教授的AARON程序 2012年：吴恩达和Jeff Dean用1.6万CPU生成猫脸 2014年：对抗生成网络GAN 2015年：《Deep …</content></entry></search>